{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install crewai\n",
    "# !pip install python-dotenv\n",
    "# !pip install crewai-tools\n",
    "# !pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from crewai_tools import BaseTool\n",
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#  LLM gpt-3.5-turbo\n",
    "\n",
    "llm35 = ChatOpenAI(temperature=0.0,\n",
    "                 model=\"gpt-3.5-turbo\",\n",
    "                 max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "loader = PyPDFLoader(\"example_data/attention.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "# Load, chunk and index the contents of the blog.\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USing in-memory document store\n",
    "# %pip install --upgrade --quiet  \"docarray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "# Upsert documents in vector store\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n"
     ]
    }
   ],
   "source": [
    "query = \"Positional encoding\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# print(prompt)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm35\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# OR you can have retrieval QA chain\n",
    "\n",
    "# Retrieval qa chain\n",
    "handler =  StdOutCallbackHandler()\n",
    "\n",
    "qa_d35 = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm35,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    callbacks=[handler],\n",
    "                                    retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The attention mechanism is a function that maps a query and a set of key-value pairs to an output, where all components (query, keys, values, and output) are vectors. It computes an output as a weighted sum based on the relevance of the key-value pairs to the query. In the context provided, the attention mechanism is a crucial part of sequence modeling and transduction models, allowing for the modeling of dependencies without considering their distance in input or output sequences. The Transformer model architecture proposed in the context relies entirely on the attention mechanism to draw global dependencies between input and output, eliminating the need for recurrence in traditional neural network architectures.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke qa_chain gpt-3.5-turbo\n",
    "\n",
    "qa_d35.invoke({\"query\": \"What is the attention mechanism?\"})[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The attention mechanism is a key component in sequence transduction models, connecting encoder and decoder through weighted sums of key-value pairs based on a query. It allows for modeling dependencies without regard to their distance in input or output sequences. The Transformer model architecture relies solely on attention mechanisms, eliminating the need for recurrence or convolutions.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is the attention mechanism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LLM gpt-4o\n",
    "\n",
    "llm4o = ChatOpenAI(temperature=0.0,\n",
    "                 model=\"gpt-4o\",\n",
    "                 max_tokens=512)\n",
    "\n",
    "# Retrieval qa chain\n",
    "qa_db4o = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm4o,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    callbacks=[handler],\n",
    "                                    retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The attention mechanism is a function that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, with the weights assigned to each value determined by a compatibility function of the query with the corresponding key. This mechanism allows the model to focus on different parts of the input sequence when producing each part of the output sequence, enabling it to model dependencies without regard to their distance in the input or output sequences.\n"
     ]
    }
   ],
   "source": [
    "print(qa_db4o.invoke({\"query\": \"What is the attention mechanism?\"})[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Certainly! To provide a comprehensive analysis of the research paper, I'll break it down into several key components:\n",
      "\n",
      "### Title and Authors\n",
      "The paper is titled \"Building a large annotated corpus of English: The Penn Treebank\" and is authored by Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\n",
      "\n",
      "### Abstract\n",
      "The abstract is not provided in the context, but typically, it would summarize the main objectives, methods, results, and conclusions of the paper.\n",
      "\n",
      "### Introduction\n",
      "The introduction likely sets the stage by discussing the importance of annotated corpora in computational linguistics and natural language processing (NLP). It probably highlights the gap that the Penn Treebank aims to fill and outlines the main contributions of the work.\n",
      "\n",
      "### Objectives\n",
      "The primary objective of the paper is to build a large, annotated corpus of English text, which can be used for various NLP tasks such as parsing, part-of-speech tagging, and syntactic analysis.\n",
      "\n",
      "### Methodology\n",
      "The methodology section would detail how the corpus was constructed. This includes:\n",
      "- **Data Collection**: The sources of the text data, which might include newspapers, books, and other written material.\n",
      "- **Annotation Process**: The process of annotating the text, including the guidelines followed and the tools used.\n",
      "- **Quality Control**: Measures taken to ensure the accuracy and consistency of the annotations.\n",
      "\n",
      "### Results\n",
      "The results section would present the size of the corpus (about 40K training sentences as mentioned), the types of annotations included, and any metrics used to evaluate the quality of the annotations.\n",
      "\n",
      "### Discussion\n",
      "The discussion might explore the implications of the Penn Treebank for the field of NLP. It could discuss how the corpus has been or could be used in various applications, such as training machine learning models for parsing and other linguistic tasks.\n",
      "\n",
      "### Conclusion\n",
      "The conclusion would summarize the main findings and contributions of the paper. It might also suggest directions for future research or improvements to the corpus.\n",
      "\n",
      "### References\n",
      "The paper cites various other works, indicating a well-researched background. For example, it references works on parsing, self-training, and attention models, showing the breadth of its relevance to different areas in NLP.\n",
      "\n",
      "### Impact\n",
      "The Penn Treebank has had a significant impact on the field of NLP. It has been widely used for training and evaluating various NLP models and has contributed to advancements in parsing, part-of-speech tagging, and other linguistic tasks.\n",
      "\n",
      "### Strengths\n",
      "- **Comprehensive Annotation**: The detailed and high-quality annotations make it a valuable\n"
     ]
    }
   ],
   "source": [
    "print(qa_db4o.invoke({\"query\": \"Can you analyze the research paper?\"})[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrewAI Agents and Tools\n",
    "\n",
    "The following steps are performed:\n",
    "\n",
    "- Search the research paper in the vector store as tool\n",
    "- Provide a detailed report with the researcher agent\n",
    "- Provide a short summary with the writer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import BaseTool\n",
    "# Create custom tool to use the vector store\n",
    "\n",
    "class MyCustomTool(BaseTool):\n",
    "    name: str = \"Research Paper\"\n",
    "    description: str = \"Research Paper\"\n",
    "\n",
    "    def _run(self, argument: str) -> str:\n",
    "        return qa_db4o.run(\"Can you analyze the research paper?\")\n",
    "\n",
    "tool_instance = MyCustomTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 08:08:32,505 - 140645041369728 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Computer Science Researcher\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Conduct an in-depth analysis of the published research paper.\n",
      "                    Identify the key components and try to understand how each works.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to conduct an in-depth analysis of the research paper, understand the key components, and interpret how each works. To do this, I will use the Research Paper tool to access the entire content of the research paper.\n",
      "\n",
      "Action: Research Paper\n",
      "Action Input: {\"argument\": \"The Published Research Paper\"}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[95m \n",
      "\n",
      "Certainly! To provide a comprehensive analysis of the research paper, I'll break it down into several key components: objectives, methodology, results, and conclusions. However, since the context provided is fragmented and does not specify a single research paper, I'll focus on the general themes and findings from the snippets available.\n",
      "\n",
      "### Objectives\n",
      "The primary objectives of the research papers mentioned seem to revolve around advancements in natural language processing (NLP) and machine learning, particularly in areas such as parsing, attention mechanisms, and language modeling. For instance:\n",
      "- **Penn Treebank (Marcus et al., 1993)**: Building a large annotated corpus of English to aid in computational linguistics.\n",
      "- **Self-training for parsing (McClosky et al., 2006)**: Improving parsing accuracy through self-training methods.\n",
      "- **Decomposable attention model (Parikh et al., 2016)**: Enhancing attention mechanisms in NLP tasks.\n",
      "- **Abstractive summarization (Paulus et al., 2017)**: Developing models for generating summaries of text.\n",
      "- **Tree annotation (Petrov et al., 2006)**: Creating accurate, compact, and interpretable tree annotations.\n",
      "\n",
      "### Methodology\n",
      "The methodologies employed in these papers vary but generally include:\n",
      "- **Corpus Building and Annotation**: The Penn Treebank project involved creating a large annotated corpus, which is foundational for many NLP tasks.\n",
      "- **Self-Training**: McClosky et al. used self-training techniques to improve parsing accuracy by iteratively retraining the model on its own predictions.\n",
      "- **Attention Mechanisms**: Parikh et al. proposed a decomposable attention model to improve the performance of NLP tasks by focusing on relevant parts of the input.\n",
      "- **Reinforcement Learning**: Paulus et al. used deep reinforcement learning for abstractive summarization, allowing the model to generate more coherent and contextually appropriate summaries.\n",
      "- **Tree Annotation**: Petrov et al. focused on learning tree annotations that are both accurate and interpretable, which is crucial for syntactic parsing.\n",
      "\n",
      "### Results\n",
      "The results from these studies have significantly impacted the field of NLP:\n",
      "- **Penn Treebank**: Provided a valuable resource for training and evaluating various NLP models.\n",
      "- **Self-Training for Parsing**: Demonstrated that self-training can effectively improve parsing accuracy, making it a valuable technique for syntactic analysis.\n",
      "- **Decomposable Attention Model**: Showed that decomposable attention can enhance the performance of models on tasks like textual entailment and question answering.\n",
      "-\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m**Abstractive Summarization**: Proved that deep reinforcement learning can be successfully applied to generate abstractive summaries, improving the coherence and quality of generated text.\n",
      "- **Tree Annotation**: Established that learning accurate, compact, and interpretable tree annotations can improve the performance of syntactic parsing models.\n",
      "\n",
      "### Conclusions\n",
      "The conclusions drawn from these papers underscore the importance and potential of NLP technologies. They highlight the necessity of continual innovation in NLP methodologies, such as improving attention mechanisms and utilizing reinforcement learning for tasks like abstractive summarization. They also emphasize the importance of corpus building and annotation, as evidenced by the impact of the Penn Treebank project.\n",
      "\n",
      "Thought: I now have a comprehensive understanding of the key components and insights from the research papers mentioned. \n",
      "Final Answer: The research papers primarily revolve around advancements in natural language processing (NLP) and machine learning. The key components are objectives, methodologies, results, and conclusions. The objectives included advancements in parsing, attention mechanisms, and language modeling. Methodologies varied but generally involved corpus building and annotation, self-training, attention mechanisms, reinforcement learning, and tree annotation. The results of these studies have significantly impacted the field of NLP, proving the effectiveness of the methodologies employed. The conclusions underscored the importance and potential of NLP technologies, highlighting the necessity of continual innovation in NLP methodologies and the importance of corpus building and annotation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Computer Science Researcher] Task output: The research papers primarily revolve around advancements in natural language processing (NLP) and machine learning. The key components are objectives, methodologies, results, and conclusions. The objectives included advancements in parsing, attention mechanisms, and language modeling. Methodologies varied but generally involved corpus building and annotation, self-training, attention mechanisms, reinforcement learning, and tree annotation. The results of these studies have significantly impacted the field of NLP, proving the effectiveness of the methodologies employed. The conclusions underscored the importance and potential of NLP technologies, highlighting the necessity of continual innovation in NLP methodologies and the importance of corpus building and annotation.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Computer Science Writer\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Extract the most important topics and key components of the report\n",
      "                    developed by the Computer Science Researcher.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now can give a great answer\n",
      "\n",
      "Final Answer:\n",
      "The research paper discusses advancements in the field of Natural Language Processing (NLP) and machine learning, with focus on parsing, attention mechanisms, and language modeling. The methodologies used are diverse, incorporating techniques such as corpus building and annotation, self-training, attention mechanisms, reinforcement learning, and tree annotation. The results demonstrate that these methods have had a substantial impact on the NLP field, validating their effectiveness. The paper concludes by emphasizing the importance and potential of NLP technologies and the need for continuous innovation in methodologies. It also underscores the significance of corpus building and annotation in developing effective NLP systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Computer Science Writer] Task output: The research paper discusses advancements in the field of Natural Language Processing (NLP) and machine learning, with focus on parsing, attention mechanisms, and language modeling. The methodologies used are diverse, incorporating techniques such as corpus building and annotation, self-training, attention mechanisms, reinforcement learning, and tree annotation. The results demonstrate that these methods have had a substantial impact on the NLP field, validating their effectiveness. The paper concludes by emphasizing the importance and potential of NLP technologies and the need for continuous innovation in methodologies. It also underscores the significance of corpus building and annotation in developing effective NLP systems.\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "# Agents and Tools\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "### RESEARCHER ###\n",
    "\n",
    "# Agent\n",
    "reasarcher = Agent(\n",
    "    role=\"Computer Science Researcher\",\n",
    "    goal='Understanding the published research paper and extract the main insights',\n",
    "    backstory=\"\"\"You work at a Computer Science school.\n",
    "                  A new paper has been published and you need to extract\n",
    "                  the main information and provide the main insights to\n",
    "                  the students.You have a knack for dissecting complex\n",
    "                  research papers\"\"\",\n",
    "    verbose=True,\n",
    "    tools=[tool_instance],\n",
    "    allow_delegation=False\n",
    "    )\n",
    "\n",
    "\n",
    "# Tasks\n",
    "task_research = Task(\n",
    "    description=\"\"\"Conduct an in-depth analysis of the published research paper.\n",
    "                    Identify the key components and try to understand how each works.\"\"\",\n",
    "    expected_output=\"\"\"Your final answer must be a full analysis report of the research paper\"\"\",\n",
    "    agent=reasarcher,\n",
    "    output_file='researcher_analysis.md' \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "### WRITER ###\n",
    "\n",
    "# Agent\n",
    "writer = Agent(\n",
    "    role=\"Computer Science Writer\",\n",
    "    goal= 'Craft easy to understand content for Computer Science students',\n",
    "    backstory=\"\"\"You are working on writing a summary of a research paper.\n",
    "                  Use the content of the Computer Science Researcher to develop\n",
    "                  a short comprehensive summary of the research paper\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "# Tasks\n",
    "task_writer = Task(\n",
    "    description=\"\"\"Extract the most important topics and key components of the report\n",
    "                    developed by the Computer Science Researcher.\"\"\",\n",
    "    expected_output=\"\"\"Your final answer must be a short comprehensive summary \n",
    "                        to understand report for Computer Science students\"\"\",\n",
    "    agent=writer,\n",
    "    output_file='writer_summary.md' \n",
    "\n",
    ")\n",
    "\n",
    "# CREWAI KICKOFF\n",
    "\n",
    "Crew = Crew(\n",
    "    agents=[reasarcher, writer],\n",
    "    tasks=[task_research, task_writer],\n",
    "    verbose=2)\n",
    "\n",
    "result = Crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper discusses advancements in the field of Natural Language Processing (NLP) and machine learning, with focus on parsing, attention mechanisms, and language modeling. The methodologies used are diverse, incorporating techniques such as corpus building and annotation, self-training, attention mechanisms, reinforcement learning, and tree annotation. The results demonstrate that these methods have had a substantial impact on the NLP field, validating their effectiveness. The paper concludes by emphasizing the importance and potential of NLP technologies and the need for continuous innovation in methodologies. It also underscores the significance of corpus building and annotation in developing effective NLP systems.\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
